<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ByteCat</title>
  
  <subtitle>探索数据的世界</subtitle>
  <link href="https://smallchao.github.io/atom.xml" rel="self"/>
  
  <link href="https://smallchao.github.io/"/>
  <updated>2022-03-28T11:09:26.103Z</updated>
  <id>https://smallchao.github.io/</id>
  
  <author>
    <name>Victor Wu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>pytorch实现A2C</title>
    <link href="https://smallchao.github.io/1095588328/"/>
    <id>https://smallchao.github.io/1095588328/</id>
    <published>2021-05-30T01:30:00.000Z</published>
    <updated>2022-03-28T11:09:26.103Z</updated>
    
    
    <summary type="html">pytorch实现优势动作评论算法(Advantage Actor Critic，A2C)。</summary>
    
    
    
    <category term="l" scheme="https://smallchao.github.io/categories/l/"/>
    
    
    <category term="S" scheme="https://smallchao.github.io/tags/S/"/>
    
  </entry>
  
  <entry>
    <title>Actor-Critic算法</title>
    <link href="https://smallchao.github.io/3806206734/"/>
    <id>https://smallchao.github.io/3806206734/</id>
    <published>2021-05-30T01:00:00.000Z</published>
    <updated>2021-11-23T08:36:21.941Z</updated>
    
    
    <summary type="html">Actor-Critic算法由两部分组成：Actor和Critic。其中Actor用的是Policy Gradient，Critic用的是Q-learning，所以它实际上是策略迭代法和价值迭代法的结合。</summary>
    
    
    
    <category term="l" scheme="https://smallchao.github.io/categories/l/"/>
    
    
  </entry>
  
  <entry>
    <title>pytorch实现优先级经验回放机制</title>
    <link href="https://smallchao.github.io/1445223830/"/>
    <id>https://smallchao.github.io/1445223830/</id>
    <published>2021-05-28T01:00:00.000Z</published>
    <updated>2022-03-28T11:09:44.108Z</updated>
    
    
    <summary type="html">采用优先级经验回放机制（Prioritized Experience Replay）可以让智能体从过去经验中更高效地学习。</summary>
    
    
    
    <category term="l" scheme="https://smallchao.github.io/categories/l/"/>
    
    
    <category term="S" scheme="https://smallchao.github.io/tags/S/"/>
    
  </entry>
  
  <entry>
    <title>pytorch实现Dueling DQN</title>
    <link href="https://smallchao.github.io/3426131253/"/>
    <id>https://smallchao.github.io/3426131253/</id>
    <published>2021-05-27T01:00:00.000Z</published>
    <updated>2022-03-28T11:10:47.850Z</updated>
    
    
    <summary type="html">Dueling DQN是DQN的改进，通过调整网络结构使得网络可以学到更为准确的状态价值的估值，从而更有效地找到好的策略（学习所需的试验轮数更少）。</summary>
    
    
    
    <category term="l" scheme="https://smallchao.github.io/categories/l/"/>
    
    
    <category term="S" scheme="https://smallchao.github.io/tags/S/"/>
    
  </entry>
  
  <entry>
    <title>pytorch实现Double DQN</title>
    <link href="https://smallchao.github.io/4099349118/"/>
    <id>https://smallchao.github.io/4099349118/</id>
    <published>2021-05-22T01:00:00.000Z</published>
    <updated>2022-03-28T11:11:01.292Z</updated>
    
    
    <summary type="html">Double DQN是DQN的改进，通过解耦目标Q值动作的选择和目标Q值的计算这两步，来达到消除过度估计的问题。</summary>
    
    
    
    <category term="l" scheme="https://smallchao.github.io/categories/l/"/>
    
    
    <category term="S" scheme="https://smallchao.github.io/tags/S/"/>
    
  </entry>
  
  <entry>
    <title>pytorch实现DQN</title>
    <link href="https://smallchao.github.io/2011684826/"/>
    <id>https://smallchao.github.io/2011684826/</id>
    <published>2021-05-20T01:00:00.000Z</published>
    <updated>2022-03-28T11:11:08.089Z</updated>
    
    
    <summary type="html">DQN，全称是Deep Q Network，是一种把Q-Learning和DNN结合起来的模型架构。</summary>
    
    
    
    <category term="l" scheme="https://smallchao.github.io/categories/l/"/>
    
    
    <category term="S" scheme="https://smallchao.github.io/tags/S/"/>
    
  </entry>
  
  <entry>
    <title>深度强化学习</title>
    <link href="https://smallchao.github.io/4188110411/"/>
    <id>https://smallchao.github.io/4188110411/</id>
    <published>2021-05-17T01:00:00.000Z</published>
    <updated>2021-11-23T08:36:21.969Z</updated>
    
    
    <summary type="html">深度强化学习是一种使用深度学习来进行强化学习的方法。</summary>
    
    
    
    <category term="l" scheme="https://smallchao.github.io/categories/l/"/>
    
    
  </entry>
  
  <entry>
    <title>gym环境下训练倒立摆</title>
    <link href="https://smallchao.github.io/699896311/"/>
    <id>https://smallchao.github.io/699896311/</id>
    <published>2021-05-15T02:00:00.000Z</published>
    <updated>2022-03-28T11:11:20.066Z</updated>
    
    
    <summary type="html">用强化学习来学习倒立摆的控制方法，这是一项比迷宫更复杂的任务。</summary>
    
    
    
    <category term="l" scheme="https://smallchao.github.io/categories/l/"/>
    
    
    <category term="S" scheme="https://smallchao.github.io/tags/S/"/>
    
  </entry>
  
  <entry>
    <title>用价值迭代法走迷宫（2）</title>
    <link href="https://smallchao.github.io/1153787513/"/>
    <id>https://smallchao.github.io/1153787513/</id>
    <published>2021-05-15T01:00:00.000Z</published>
    <updated>2022-03-28T11:11:26.318Z</updated>
    
    
    <summary type="html">实现Q学习算法（一种价值迭代算法）走迷宫。与Sarsa不同的之处在于其动作价值函数的更新公式不同。</summary>
    
    
    
    <category term="l" scheme="https://smallchao.github.io/categories/l/"/>
    
    
    <category term="S" scheme="https://smallchao.github.io/tags/S/"/>
    
  </entry>
  
  <entry>
    <title>用价值迭代法走迷宫（1）</title>
    <link href="https://smallchao.github.io/1450232215/"/>
    <id>https://smallchao.github.io/1450232215/</id>
    <published>2021-05-11T01:00:00.000Z</published>
    <updated>2022-03-28T11:11:32.958Z</updated>
    
    
    <summary type="html">实现Sarsa算法（一种价值迭代算法）走迷宫。</summary>
    
    
    
    <category term="l" scheme="https://smallchao.github.io/categories/l/"/>
    
    
    <category term="S" scheme="https://smallchao.github.io/tags/S/"/>
    
  </entry>
  
  <entry>
    <title>用策略迭代法走迷宫</title>
    <link href="https://smallchao.github.io/1256595214/"/>
    <id>https://smallchao.github.io/1256595214/</id>
    <published>2021-05-04T01:00:00.000Z</published>
    <updated>2022-03-28T11:11:38.962Z</updated>
    
    
    <summary type="html">策略迭代法是一种强化学习算法，通过不断更新策略来学习。</summary>
    
    
    
    <category term="l" scheme="https://smallchao.github.io/categories/l/"/>
    
    
    <category term="S" scheme="https://smallchao.github.io/tags/S/"/>
    
  </entry>
  
  <entry>
    <title>随机搜索走迷宫</title>
    <link href="https://smallchao.github.io/3153300902/"/>
    <id>https://smallchao.github.io/3153300902/</id>
    <published>2021-05-03T01:00:00.000Z</published>
    <updated>2022-03-30T11:31:36.155Z</updated>
    
    
    <summary type="html">作为强化学习的对照，本节中实现一个智能体，该智能体在迷宫中以随机搜索的方式朝目标前进。</summary>
    
    
    
    <category term="l" scheme="https://smallchao.github.io/categories/l/"/>
    
    
    <category term="S" scheme="https://smallchao.github.io/tags/S/"/>
    
  </entry>
  
  <entry>
    <title>强化学习构成要素</title>
    <link href="https://smallchao.github.io/808510049/"/>
    <id>https://smallchao.github.io/808510049/</id>
    <published>2021-05-02T01:00:00.000Z</published>
    <updated>2021-11-23T08:36:21.943Z</updated>
    
    
    <summary type="html">强化学习的主要构成要素包括：智能体、环境、行动、奖励。</summary>
    
    
    
    <category term="l" scheme="https://smallchao.github.io/categories/l/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习概述</title>
    <link href="https://smallchao.github.io/2012735660/"/>
    <id>https://smallchao.github.io/2012735660/</id>
    <published>2021-05-01T01:00:00.000Z</published>
    <updated>2021-11-23T08:36:21.951Z</updated>
    
    
    <summary type="html">强化学习（Reinforcement Learning, RL），又称再励学习、评价学习或增强学习，主要用于时变系统控制规则构建和对战博弈策略构建。</summary>
    
    
    
    <category term="l" scheme="https://smallchao.github.io/categories/l/"/>
    
    
  </entry>
  
  <entry>
    <title>半监督学习算法（3）-- 聚类</title>
    <link href="https://smallchao.github.io/2652695742/"/>
    <id>https://smallchao.github.io/2652695742/</id>
    <published>2021-04-03T02:00:00.000Z</published>
    <updated>2022-03-28T11:11:51.350Z</updated>
    
    
    <summary type="html">半监督聚类利用已标记的数据样本对聚类过程进行指导，提高了无监督学习的准确率。</summary>
    
    
    
    <category term="k" scheme="https://smallchao.github.io/categories/k/"/>
    
    
    <category term="S" scheme="https://smallchao.github.io/tags/S/"/>
    
  </entry>
  
  <entry>
    <title>半监督学习算法（2）-- 分类</title>
    <link href="https://smallchao.github.io/3905662131/"/>
    <id>https://smallchao.github.io/3905662131/</id>
    <published>2021-04-02T02:00:00.000Z</published>
    <updated>2022-03-28T11:11:57.217Z</updated>
    
    
    <summary type="html">半监督分类是在无类标签的样例的帮助下训练有类标签的样本，获得比只用有类标签的样本训练得到的分类器性能更优的分类器，弥补有类标签的样本不足的缺陷。</summary>
    
    
    
    <category term="k" scheme="https://smallchao.github.io/categories/k/"/>
    
    
    <category term="S" scheme="https://smallchao.github.io/tags/S/"/>
    
  </entry>
  
  <entry>
    <title>半监督学习算法（1）</title>
    <link href="https://smallchao.github.io/660866816/"/>
    <id>https://smallchao.github.io/660866816/</id>
    <published>2021-04-01T02:00:00.000Z</published>
    <updated>2021-11-23T08:36:21.848Z</updated>
    
    
    <summary type="html">无监督学习只利用未标记的样本集，而监督学习则只利用标记的样本集进行学习。但在很多实际问题中，只有少量的带有标记的数据，因为对数据进行标记的代价有时很高，比如在生物学中，对某种蛋白质的结构分析或者功能鉴定，可能会花上生物学家很多年的工作，而大量的未标记的数据却很容易得到，这就促使能同时利用标记样本和未标记样本的半监督学习技术迅速发展起来。</summary>
    
    
    
    <category term="k" scheme="https://smallchao.github.io/categories/k/"/>
    
    
  </entry>
  
  <entry>
    <title>用LSTM预测股票价格</title>
    <link href="https://smallchao.github.io/564105494/"/>
    <id>https://smallchao.github.io/564105494/</id>
    <published>2021-01-15T07:00:00.000Z</published>
    <updated>2022-03-28T11:12:09.193Z</updated>
    
    
    <summary type="html">使用LSTM预测股票价格的例子，数据来自tushare。</summary>
    
    
    
    <category term="i" scheme="https://smallchao.github.io/categories/i/"/>
    
    
    <category term="S" scheme="https://smallchao.github.io/tags/S/"/>
    
  </entry>
  
  <entry>
    <title>Auto-Arima工具包</title>
    <link href="https://smallchao.github.io/214610947/"/>
    <id>https://smallchao.github.io/214610947/</id>
    <published>2021-01-15T02:00:00.000Z</published>
    <updated>2022-03-28T11:12:15.557Z</updated>
    
    
    <summary type="html">通过Auto-Arima，可以让模型自己选择参数，而无需人工慢慢调参。</summary>
    
    
    
    <category term="i" scheme="https://smallchao.github.io/categories/i/"/>
    
    
    <category term="S" scheme="https://smallchao.github.io/tags/S/"/>
    
  </entry>
  
  <entry>
    <title>自回归模型</title>
    <link href="https://smallchao.github.io/611143796/"/>
    <id>https://smallchao.github.io/611143796/</id>
    <published>2021-01-05T02:00:00.000Z</published>
    <updated>2022-03-28T11:12:22.002Z</updated>
    
    
    <summary type="html">自回归（Autoregression，AR）是指用前期数据来预测后期数据的回归模型。它的逻辑很简单，但对特定的时间序列问题能够做出相当准确的预测。</summary>
    
    
    
    <category term="i" scheme="https://smallchao.github.io/categories/i/"/>
    
    
    <category term="S" scheme="https://smallchao.github.io/tags/S/"/>
    
  </entry>
  
</feed>
