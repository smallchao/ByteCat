<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.ico?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.ico?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    }
  };
</script>

  <meta name="description" content="自然语言处理的发展大致经历了4个阶段：1956年以前的萌芽期；1957-1970年的快速发展期；1971 -1993年的低谷的发展期和1994年至今的复苏融合期。">
<meta name="keywords" content="数据科学 比特猫 Byte猫">
<meta property="og:type" content="article">
<meta property="og:title" content="自然语言处理的发展历程">
<meta property="og:url" content="https://smallchao.github.io/10/2自然语言处理的发展历程/index.html">
<meta property="og:site_name" content="Byte猫">
<meta property="og:description" content="自然语言处理的发展大致经历了4个阶段：1956年以前的萌芽期；1957-1970年的快速发展期；1971 -1993年的低谷的发展期和1994年至今的复苏融合期。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://smallchao.github.io/upload_images/9210113-cd7066a8eed422d5.png">
<meta property="og:image" content="https://smallchao.github.io/upload_images/9210113-d01a80380f43a8c0.png">
<meta property="og:image" content="https://smallchao.github.io/upload_images/9210113-dbc0597e18d03a07.png">
<meta property="og:image" content="https://smallchao.github.io/upload_images/9210113-3af75c66186a2635.png">
<meta property="og:image" content="https://smallchao.github.io/upload_images/9210113-dd17f804a436996e.png">
<meta property="og:image" content="https://smallchao.github.io/upload_images/9210113-ea006939788c5731.png">
<meta property="og:image" content="https://smallchao.github.io/upload_images/9210113-398eef5c8b18b85e.png">
<meta property="og:image" content="https://smallchao.github.io/upload_images/9210113-e6b83619b3843340.png">
<meta property="og:image" content="https://smallchao.github.io/upload_images/9210113-2b0d1acef1989dab.png">
<meta property="og:image" content="https://smallchao.github.io/upload_images/9210113-51d816efc5df6bb4.png">
<meta property="og:image" content="https://smallchao.github.io/upload_images/9210113-f1b75e4ae0c5c351.png">
<meta property="og:image" content="https://smallchao.github.io/upload_images/9210113-f4e67fd2e36f7ccf.png">
<meta property="og:updated_time" content="2019-09-02T04:10:47.327Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="自然语言处理的发展历程">
<meta name="twitter:description" content="自然语言处理的发展大致经历了4个阶段：1956年以前的萌芽期；1957-1970年的快速发展期；1971 -1993年的低谷的发展期和1994年至今的复苏融合期。">
<meta name="twitter:image" content="https://smallchao.github.io/upload_images/9210113-cd7066a8eed422d5.png">
  <link rel="canonical" href="https://smallchao.github.io/10/2自然语言处理的发展历程/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>自然语言处理的发展历程 | Byte猫</title>
  <meta name="generator" content="Hexo 3.9.0">
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <div class="container use-motion">
    <div class="headband"></div>
	<a href="https://github.com/smallchao" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Byte猫</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">唤醒数据的价值</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-首页">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-数据结构">
      
    

    <a href="/categories/0" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>数据结构</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-基础应用">
      
    

    <a href="/categories/1" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>基础应用</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-数据存储">
      
    

    <a href="/categories/2" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>数据存储</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-数据可视化">
      
    

    <a href="/categories/3" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>数据可视化</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-数据科学知识">
      
    

    <a href="/categories/4" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>数据科学知识</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-机器学习原理">
      
    

    <a href="/categories/5" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>机器学习原理</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-经典机器学习">
      
    

    <a href="/categories/6" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>经典机器学习</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-半监督学习与强化学习">
      
    

    <a href="/categories/7" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>半监督学习与强化学习</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-神经网络与深度学习">
      
    

    <a href="/categories/8" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>神经网络与深度学习</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-【分支】个性化推荐">
      
    

    <a href="/categories/9" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>【分支】个性化推荐</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-【分支】自然语言处理">
      
    

    <a href="/categories/10" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>【分支】自然语言处理</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-【分支】计算机视觉">
      
    

    <a href="/categories/11" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>【分支】计算机视觉</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-模型工程化">
      
    

    <a href="/categories/12" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>模型工程化</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content page-post-detail">
            

  <div id="posts" class="posts-expand">
    

  <article class="post" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://smallchao.github.io/10/2自然语言处理的发展历程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Wu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Byte猫">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">自然语言处理的发展历程

          
        </h1>

        <div class="post-meta">
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-09-02 12:10:47" itemprop="dateModified" datetime="2019-09-02T12:10:47+08:00">2019-09-02</time>
              </span>
            
          

          
            <div class="post-description">自然语言处理的发展大致经历了4个阶段：1956年以前的萌芽期；1957-1970年的快速发展期；1971 -1993年的低谷的发展期和1994年至今的复苏融合期。</div>
          

        </div>
      </header>

    
    
    
	
    <div class="post-body" itemprop="articleBody">

      
	  
	  
        <p>NLP的发展趋势：规则—&gt;统计—&gt;深度学习。<br>自然语言处理的发展大致经历了4个阶段：1956年以前的萌芽期；1957-1970年的快速发展期；1971 -1993年的低谷的发展期和1994年至今的复苏融合期。</p><a id="more"></a>
<h3 id="一、萌芽期-1956年以前"><a href="#一、萌芽期-1956年以前" class="headerlink" title="一、萌芽期(1956年以前)"></a>一、萌芽期(1956年以前)</h3><p>1956年以前，可以看作自然语言处理的基础研究阶段。一方面，人类文明经过了几千年的发展，积累了大量的数学、语言学和物理学知识。这些知识不仅是计算机诞生的必要条件，同时也是自然语言处理的理论基础。另一方面，阿兰·图灵在1936年首次提出了“图灵机”的概念。“图灵机”作为计算机的理论基础，促使了1946年电子计算机的诞生。而电子计算机的诞生又为机器翻译和随后的自然语言处理提供了物质基础。<br>由于来自机器翻译的社会需求，这一时期也进行了许多自然语言处理的基础研究。1948年Shannon把离散马尔可夫过程的概率模型应用于描述语言的自动机。接着，他又把热力学中“熵”(entropy)的概念引用于语言处理的概率算法中。上世纪50年代初，Kleene研究了有限自动机和正则表达式。1956年，Chomsky又提出了上下文无关语法，并把它运用到自然语言处理中。他们的工作直接引起了基于规则和基于概率这两种不同的自然语言处理技术的产生。而这两种不同的自然语言处理方法，又引发了数十年有关基于规则方法和基于概率方法孰优孰劣的争执。<br>另外，这一时期还取得了一些令人瞩目的研究成果。比如，1946年Köenig进行了关于声谱的研究。1952年Bell实验室语音识别系统的研究。1956年人工智能的诞生为自然语言处理翻开了新的篇章。这些研究成果在后来的数十年中逐步与自然语言处理中的其他技术相结合。这种结合既丰富了自然语言处理的技术手段，同时也拓宽了自然语言处理的社会应用面。</p>
<h3 id="二、快速发展期-1957-1970"><a href="#二、快速发展期-1957-1970" class="headerlink" title="二、快速发展期(1957-1970)"></a>二、快速发展期(1957-1970)</h3><p>自然语言处理在这一时期很快融入了人工智能的研究领域中。由于有基于规则和基于概率这两种不同方法的存在，自然语言处理的研究在这一时期分为了两大阵营。一个是基于规则方法的符号派(symbolic)，另一个是采用概率方法的随机派(stochastic)。<br>这一时期，两种方法的研究都取得了长足的发展。从50年代中期开始到60年代中期，以Chomsky为代表的符号派学者开始了形式语言理论和生成句法的研究，60年代末又进行了形式逻辑系统的研究。而随机派学者采用基于贝叶斯方法的统计学研究方法，在这一时期也取得了很大的进步。但由于在人工智能领域中，这一时期多数学者注重研究推理和逻辑问题，只有少数来自统计学专业和电子专业的学者在研究基于概率的统计方法和神经网络，所<br>以，在这一时期中，基于规则方法的研究势头明显强于基于概率方法的研究势头。<br>这一时期的重要研究成果包括1959年宾夕法尼亚大学研制成功的TDAP系统，布朗美国英语语料库的建立等。1967年美国心理学家Neisser提出认知心理学的概念，直接把自然语言处理与人类的认知联系起来了。</p>
<h3 id="三、低速的发展期-1971-1993"><a href="#三、低速的发展期-1971-1993" class="headerlink" title="三、低速的发展期(1971 -1993)"></a>三、低速的发展期(1971 -1993)</h3><p>随着研究的深入，由于人们看到基于自然语言处理的应用并不能在短时间内得到解决，而一连串的新问题又不断地涌现，于是，许多人对自然语言处理的研究丧失了信心。从70年代开始，自然语言处理的研究进入了低谷时期。<br>但尽管如此，一些发达国家的研究人员依旧不依不挠地继续着他们的研究。由于他们的出色工作，自然语言处理在这一低谷时期同样取得了一些成果。70年代，基于隐马尔可夫模型(Hidden Markov Model, HMM)的统计方法在语音识别领域获得成功。80年代初，话语分析(Discourse Analysis)也取得了重大进展。之后，由于自然语言处理研究者对于过去的研究进行了反思，有限状态模型和经验主义研究方法也开始复苏。</p>
<h3 id="四、复苏融合期-1994年至今"><a href="#四、复苏融合期-1994年至今" class="headerlink" title="四、复苏融合期(1994年至今)"></a>四、复苏融合期(1994年至今)</h3><p>90年代中期以后，有两件事从根本上促进了自然语言处理研究的复苏与发展。一件事是90年代中期以来，计算机的速度和存储量大幅增加，为自然语言处理改善了物质基础，使得语音和语言处理的商品化开发成为可能；另一件事是1994年Internet商业化和同期网络技术的发展使得基于自然语言的信息检索和信息抽取的需求变得更加突出。<br>2000年之后的几个里程碑事件：<br>2001年 - 神经语言模型<br>2008年 - 多任务学习<br>2013年 - Word嵌入<br>2013年 - NLP的神经网络<br>2014年 - 序列到序列模型<br>2015年 - 注意力机制<br>2015年 - 基于记忆的神经网络<br>2018年 - 预训练语言模型</p>
<h4 id="（1）2001年——神经语言模型（Neurallanguage-models）"><a href="#（1）2001年——神经语言模型（Neurallanguage-models）" class="headerlink" title="（1）2001年——神经语言模型（Neurallanguage models）"></a>（1）2001年——神经语言模型（Neurallanguage models）</h4><p>语言模型解决的是在给定已出现词语的文本中，预测下一个单词的任务。这可以算是最简单的语言处理任务，但却有许多具体的实际应用，例如智能键盘、电子邮件回复建议等。当然，语言模型的历史由来已久。经典的方法基于 n-grams 模型（利用前面 n 个词语预测下一个单词），并利用平滑操作处理不可见的 n-grams。<br>第一个神经语言模型，前馈神经网络（feed-forward neuralnetwork），是 Bengio 等人于 2001 年提出的。</p>
<p><img src="/upload_images/9210113-cd7066a8eed422d5.png" alt></p>
<p>以某词语之前出现的n个词语作为输入向量。今天，这样的向量被称为大家熟知的词嵌入（word embeddings）。这些单词嵌入被连接并馈入隐藏层，然后将其输出提供给softmax层。<br>最近，前馈神经网络已经被用于语言建模的递归神经网络和长期短期记忆网络所取代。近年来已经提出了许多扩展经典LSTM的新语言模型。尽管有这些发展，但经典的LSTM仍然是一个强大的基础模型。更好地理解语言模型究竟捕捉了哪些信息，也是当今一个活跃的研究领域。<br>语言建模是无监督学习的一种形式，Yann LeCun也将预测性学习称为获取常识的先决条件。关于语言建模最值得注意的方面可能是，尽管它很简单，但它是本文讨论的许多后期进展的核心：<br>–Word嵌入：word2vec的目标是简化语言建模。<br>–序列到序列模型：这种模型通过一次预测一个单词来生成输出序列。<br>–预训练语言模型：这些方法使用语言模型中的表示来进行转移学习。<br>这意味着NLP的许多重要最新进展可以归结为一种语言建模形式。为了做“真正的”自然语言理解，需要新的方法和模型。</p>
<p>####（2）2008年——多任务学习（Multi-tasklearning）</p>
<p>多任务学习是在多个任务下训练的模型之间共享参数的一般方法。在神经网络中，这可以通过绑定不同层的权重来轻松完成。多任务学习的想法于1993年由Rich Caruana首次提出，并应用于道路跟踪和肺炎预测（Caruana，1998）。直观地说，多任务学习鼓励模型学习对许多任务有效的表征描述。这对于学习一般的低级表示，集中模型的注意力或在有限量的训练数据的设置中特别有用。<br>Collobert在2008年首次将多任务学习应用于NLP的神经网络。在这一框架下，词嵌入矩阵被两个在不同任务下训练的模型共享.</p>
<p><img src="/upload_images/9210113-d01a80380f43a8c0.png" alt></p>
<p>共享单词嵌入使模型能够在单词嵌入矩阵中协作和共享一般的低级信息，这通常构成模型中最大数量的参数。Collobert和Weston在2008年的论文中证明了它在多任务学习中的应用。它引领了诸如预训练单词嵌入和使用卷积神经网络（CNN）之类的方法，这些方法仅在过去几年中被广泛采用。他们也因此获得了2018年机器学习国际会议（ICML）的“时间测试”奖。<br>多任务学习现在用于各种NLP任务，并且利用现有或“人工”任务已成为NLP指令集中的有用工具。虽然通常预先定义参数的共享，但是在优化过程期间也可以学习不同的共享模式。随着模型越来越多地评估多项任务以评估其泛化能力，多任务学习越来越重要，最近又有提出了多任务学习的专用基准。</p>
<p>####（3）2013年 - 词嵌入（Word embeddings）</p>
<p>词嵌入在2001年首次出现。而Mikolov等人在2013年作出的主要创新——是通过删除隐藏层和近似目标来使这些单词嵌入的训练更有效。虽然这些变化本质上很简单，但它们与高效的word2vec（word to vector，用来产生词向量的相关模型）组合在一起，使得大规模的词嵌入模型训练成为可能。<br>Word2vec有两种风格，可以在下面图中看到：CBOW（continuous bag-of-words）和skip-gram。它们的目标不同：一个基于周围的单词预测中心词，而另一个则相反。</p>
<p><img src="/upload_images/9210113-dbc0597e18d03a07.png" alt></p>
<p>虽然捕获的关系word2vec具有直观且几乎神奇的质量，但后来的研究表明word2vec没有任何固有的特殊性：通过矩阵分解也可以学习单词嵌入和通过适当的调整，像SVD和LSA这样的经典矩阵分解方法可以获得类似的结果。<br>从那时起，许多工作已经开始探索单词嵌入的不同方面（正如原始论文的引用次数所示）。尽管有许多发展，但word2ve仍然是一种流行的选择并且在今天被广泛使用。Word2vec的范围甚至超出了单词级别：带有负抽样的skip-gram，一个基于本地环境学习嵌入的方便目标，已被应用于学习句子的表示，甚至超越NLP到网络和生物序列等。<br>一个特别令人兴奋的方向是将不同语言的单词嵌入投影到同一空间中以实现（零射击）跨语言转移。越来越有可能以完全无监督的方式（至少对于类似语言）学习良好的投影，这开启了低资源语言和无监督机器翻译的应用。</p>
<h4 id="（4）2013年-用于自然语言处理的神经网络（Neural-networks-for-NLP）"><a href="#（4）2013年-用于自然语言处理的神经网络（Neural-networks-for-NLP）" class="headerlink" title="（4）2013年 - 用于自然语言处理的神经网络（Neural networks for NLP）"></a>（4）2013年 - 用于自然语言处理的神经网络（Neural networks for NLP）</h4><p>2013年和2014年标志着神经网络模型开始在NLP中被采用的时间。三种主要类型的神经网络成为使用最广泛的：循环神经网络（recurrent neural networks）、卷积神经网络（convolutionalneural networks）和结构递归神经网络（recursive neural networks）。<br>递归神经网络（RNN）是处理NLP中普遍存在的动态输入序列的理想选择。Vanilla RNNs很快被经典的长期短期记忆网络（LSTM）所取代，后者证明其对消失和爆炸梯度问题更具弹性。在2013年之前，仍然认为RNN很难训练；Ilya Sutskever的博士论文是改变这一局面的一个关键例子。LSTM细胞的可视化可以在下图中看到。双向LSTM通常用于处理左右上下文。</p>
<p><img src="/upload_images/9210113-3af75c66186a2635.png" alt></p>
<p>随着卷积神经网络（CNN）被广泛用于计算机视觉，它们也开始应用于语言（Kalchbrenner等，2014；Kim等，2014）。用于文本的卷积神经网络仅在两个维度上操作，其中滤波器仅需要沿时间维度移动。下图显示了NLP中使用的典型CNN。</p>
<p><img src="/upload_images/9210113-dd17f804a436996e.png" alt></p>
<p>卷积神经网络的一个优点是它们比RNN更容易并行化，因为每个时间步的状态仅取决于本地环境（通过卷积运算）而不是像RNN中的所有过去状态。 CNN可以使用扩张的卷积扩展到更宽的感受域，以捕捉更广泛的背景（Kalchbrenner等，2016）。CNN和LSTM也可以组合和堆叠，并且可以使用卷积来加速LSTM。<br>RNN和CNN都将语言视为一个序列。然而，从语言学的角度来看，语言本质上是等级的：单词被组成高阶短语和子句，它们本身可以根据一组生产规则递归地组合。将句子视为树而不是序列的语言启发思想产生了递归神经网络。</p>
<p><img src="/upload_images/9210113-ea006939788c5731.png" alt></p>
<p>自下而上构建序列的结构递归神经网络，与从左至右或从右至左对序列进行处理的循环神经网络相比，有着明显的不同。在树的每个节点处，通过组合子节点的表示来计算新表示。由于树也可以被视为在RNN上施加不同的处理顺序，因此LSTM自然地扩展到树形结构取代序列。<br>不仅可以扩展RNN和LSTM以使用分层结构。不仅可以根据本地语言而且可以基于语法背景来学习单词嵌入（Levy＆Goldberg，2014）；语言模型可以基于句法堆栈生成单词（Dyer等，2016）；图形卷积神经网络可以树状结构运行（Bastings等，2017）</p>
<h4 id="（5）2014年-序列到序列模型（Sequence-to-sequence-models）"><a href="#（5）2014年-序列到序列模型（Sequence-to-sequence-models）" class="headerlink" title="（5）2014年 - 序列到序列模型（Sequence-to-sequence models）"></a>（5）2014年 - 序列到序列模型（Sequence-to-sequence models）</h4><p>2014年，Sutskever等人提出了序列到序列学习，一种使用神经网络将一个序列映射到另一个序列的通用框架。在该框架中，编码器神经网络逐符号地处理句子并将其压缩成矢量表示；然后，解码器神经网络基于编码器状态逐个预测输出符号，在每个步骤中将先前预测的符号作为预测下一个的输入.</p>
<p><img src="/upload_images/9210113-398eef5c8b18b85e.png" alt></p>
<p>机器翻译成了这个框架的杀手级应用。 2016年，谷歌宣布开始用神经MT模型替换其基于单片短语的MT模型（Wu等，2016）。根据Jeff Dean的说法，这意味着用500行神经网络模型替换500,000行基于短语的机器翻译代码。<br>由于其灵活性，该框架现在是自然语言生成任务的首选框架，不同的模型承担编码器和解码器的角色。重要的是，解码器模型不仅可以以序列为条件，而且可以以任意表示为条件。这使得例如基于图片生成描述（Vinyals等人，2015），基于表格的文本（Lebret等人，2016），基于源的描述、代码更改（Loyola等，2017），以及许多其他应用程序成为可能。</p>
<p><img src="/upload_images/9210113-e6b83619b3843340.png" alt></p>
<p>序列到序列学习甚至可以应用于NLP中常见的结构化预测任务，其中输出具有特定结构。为简单起见，输出是线性化的，如下面图10中的选区解析所示。神经网络已经证明了在给予选区解析的足够数量的训练数据（Vinyals等，2015）和命名实体识别（Gillick等，2016）等的情况下，能够直接学习产生这种线性化输出的能力。</p>
<p><img src="/upload_images/9210113-2b0d1acef1989dab.png" alt></p>
<p>用于序列和解码器的编码器通常基于RNN，但是可以使用其他模型类型。新架构主要来自机器翻译的工作，机器翻译将作为序列到序列架构的培养皿。最近的模型是深度LSTM（Wu等，2016）、卷积编码器（Kalchbrenner等，2016；Gehring等，2017）、变换器（Vaswani等，2017）将在下一个讨论部分，以及LSTM和变压器的组合（Chen等，2018）。</p>
<h4 id="（6）2015年-注意力机制（Attention）"><a href="#（6）2015年-注意力机制（Attention）" class="headerlink" title="（6）2015年 - 注意力机制（Attention）"></a>（6）2015年 - 注意力机制（Attention）</h4><p>注意力（Bahdanau等，2015）是神经MT（NMT）的核心创新之一，也是使NMT模型优于基于经典短语的MT系统的关键思想。序列到序列学习的主要瓶颈是它需要将源序列的整个内容压缩成固定大小的矢量。注意通过允许解码器回顾源序列隐藏状态来减轻这种情况，然后将其作为加权平均值提供给解码器的附加输入。</p>
<p><img src="/upload_images/9210113-51d816efc5df6bb4.png" alt></p>
<p>注意力机制是神经网络机器翻译 (NMT) 的核心创新之一，广泛适用，并且可能对任何需要根据输入的某些部分做出决策的任务有用。它已被应用于句法分析（Vinyals等，2015）、阅读理解（Hermann等，2015）和单样本学习（Vinyals等，2016）等等。输入的甚至不需要是一个序列，可以包括其他表示，比如图像的描述。注意力的一个有用的副作用是，通过根据注意力量检查输入的哪些部分与特定输出相关，它提供了罕见的对模型内部运作机制的观察。</p>
<p><img src="/upload_images/9210113-f1b75e4ae0c5c351.png" alt></p>
<p>注意也不仅限于查看输入序列；自我注意可用于查看句子或文档中的周围单词以获得更多上下文敏感的单词表示。多层自我关注是Transformer架构的核心（Vaswani等，2017），这是目前最先进的NMT模型。</p>
<h4 id="（7）2015年-基于记忆的神经网络（Memory-based-networks）"><a href="#（7）2015年-基于记忆的神经网络（Memory-based-networks）" class="headerlink" title="（7）2015年 - 基于记忆的神经网络（Memory-based networks）"></a>（7）2015年 - 基于记忆的神经网络（Memory-based networks）</h4><p>注意力可以看作是模糊记忆的一种形式，其中记忆由模型的过去隐藏状态组成，模型选择从记忆中检索的内容。有关注意事项及其与内存的关联的更详细概述，请查看此文章。已经提出了许多具有更明确记忆的模型。它们有不同的变体，例如神经图灵机（Neural Turing Machines）、记忆网络（Memory Network）、端到端的记忆网络（End-to-end Memory Newtorks）、动态记忆网络（DynamicMemory Networks）、神经可微计算机（Neural Differentiable Computer）、循环实体网络（RecurrentEntity Network）。<br>通常基于与当前状态的相似性来访问存储器，类似于注意机制，并且通常可以写入和读取存储器。模型在实现和利用内存方面有所不同。例如，端到端内存网络多次处理输入并更新内存以启用多个推理步骤。神经图灵机还具有基于位置的寻址，允许他们学习简单的计算机程序，如排序。基于内存的模型通常应用于任务，其中保留较长时间跨度的信息应该是有用的，例如语言建模和阅读理解。存储器的概念非常通用：知识库或表可以用作存储器，而存储器也可以基于整个输入或其特定部分来填充。</p>
<h4 id="（8）预训练语言模型（Pretrained-language-models）"><a href="#（8）预训练语言模型（Pretrained-language-models）" class="headerlink" title="（8）预训练语言模型（Pretrained language models）"></a>（8）预训练语言模型（Pretrained language models）</h4><p>预训练的词嵌入与上下文无关，仅用于初始化模型中的第一层。最近几个月，一系列监督任务被用于预训练神经网络（Conneau等，2017；McCann等，2017； Subramanian等，2018）。相比之下，语言模型只需要未标记的文本；因此，培训可以扩展到数十亿单词的语料、新域和新语言。2015年首次提出了预训练语言模型（Dai＆Le，2015）;直到最近，它们才被证明对各种各样的任务都有益。语言模型嵌入可以用作目标模型中的特征（Peters等，2018），或者可以对目标任务数据微调语言模型（Ramachandran等，2017； Howard＆Ruder，2018）。添加语言模型嵌入比许多不同任务的最新技术有了很大的改进。</p>
<p><img src="/upload_images/9210113-f4e67fd2e36f7ccf.png" alt></p>
<p>已经展示了预训练语言模型，可以用更少的数据进行学习。由于语言模型仅需要未标记的数据，因此对于标记数据稀缺的低资源语言尤其有用。<br>其他里程碑：<br>其他一些发展不如上面提到的那么普遍，但仍然具有广泛的影响。<br>比如基于字符的描述（Character-based representations），在字符上使用CNN或LSTM来获得基于字符的单词表示是相当普遍的，特别是对于形态学丰富的语言和形态信息很重要或具有许多未知单词的任务。据我所知，基于特征的表示首先用于序列标记（Lample等，2016；Plank等，2016）。基于字符的表示减少了必须以增加的计算成本处理固定词汇表的需要，并且能够实现诸如完全基于字符的NMT之类的应用（Ling等人，2016； Lee等人，2017）。<br>对抗学习（Adversarial learning）已经全面入侵和颠覆了及其计算领域，并且在NLP中也以不同的形式使用。对抗性示例越来越广泛地被广泛使用，不仅作为探测模型和理解其失败案例的工具，而且还使它们更加强大（Jia＆Liang，2017）。 （虚拟）对抗性训练，即最坏情况的扰动（Miyato等，2017; Yasunaga等，2018）和域对抗性损失（Ganin等，2016; Kim等，2017）是有用的正规化的形式可以同样使模型更加坚稳。生成对抗网络（GAN）对于自然语言生成来说，还不是太有效（Semeniuta等，2018），但是例如在匹配分布时是有用的（Conneau等，2018）。<br>强化学习（Reinforcement learning）已经被证明对于具有时间依赖性的任务是有用的，例如在训练期间选择数据（Fang等，Wu等，2018）和建模对话（Liu等，2018）。RL对于直接优化诸如反向强化学习在奖励太复杂而无法指定的环境中可能是有用的，例如视觉叙事（Wang等，2018）。<br>非神经网络方向的里程碑：<br>在1998年以及随后的几年中，引入了FrameNet项目（Baker等，1998），这导致了语义角色标记的任务，这是一种浅层语义分析，至今仍在积极研究中。在21世纪初期，与自然语言学习会议（CoNLL）共同组织的共同任务催化了核心NLP任务的研究，如分块（Tjong Kim Sang等，2000），命名实体识别（Tjong Kim Sang等，2003），以及依赖性解析（Buchholz等，2006）等。许多CoNLL共享任务数据集仍然是当今评估的标准。<br>2001年，引入了条件随机区域（CRF; Lafferty等，2001），这是最具影响力的序列标记方法之一，在ICML 2011中获得了时间测试奖.CRF层是核心部分目前最先进的模型用于序列标记问题与标签相互依赖性，如命名实体识别（Lample等，2016）。<br>2002年，提出了双语评估替代研究（BLEU； Papineni等，2002）度量，这使得MT系统能够扩展，并且仍然是目前MT评估的标准度量。同年，引入了结构化先行者（Collins，2002），为结构化感知工作奠定了基础。在同一次会议上，引入了情感分析，这是最受欢迎和广泛研究的NLP任务之一（Pang等，2002）。这三篇论文都获得了2018年NAACL的时间测试奖。<br>2003年引入了潜在的dirichlet分配（LDA； Blei等，2003），这是机器学习中使用最广泛的技术之一，它仍然是进行主题建模的标准方法。2004年，提出了新的最大边际模型，它们更适合捕获结构化数据中的相关性而不是SVM（Taskar等，2004a； 2004b）。<br>2006年，OntoNotes（Hovy等，2006）引入了一个具有多个注释和高交互注入协议的大型多语言语料库。 OntoNotes已被用于培训和评估各种任务，例如依赖性解析和共参考解析。 Milne和Witten（2008）在2008年描述了维基百科如何用于丰富机器学习方法。到目前为止，维基百科是用于训练ML方法的最有用的资源之一，无论是用于实体链接和消歧，语言建模，作为知识库还是各种其他任务。<br>2009年，提出了远程监督的想法（Mintz等，2009）。远程监督利用来自启发式或现有知识库的信息来生成可用于从大型语料库中自动提取示例的噪声模式。远程监督已被广泛使用，并且是关系提取，信息提取和情感分析以及其他任务中的常用技术。</p>

    </div>
	

    
    
    
        
      

      <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/10/1自然语言处理几个概念/" rel="next" title="自然语言处理的几个概念">
                  <i class="fa fa-chevron-left"></i> 自然语言处理的几个概念
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/11/1计算机视觉的几个概念/" rel="prev" title="计算机视觉的几个概念">
                  计算机视觉的几个概念 <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

	
      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc" data-target="post-toc-wrap">
          文章目录
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
          站点概览
        </li>
      </ul>
	 

      <!--noindex-->
	  
      <div class="post-toc-wrap sidebar-panel">
		<!--
		-->
      </div>
	  
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.png"
      alt="Victor Wu">
  <p class="site-author-name" itemprop="name">Victor Wu</p>
  <div class="site-description motion-element" itemprop="description"></div>
</div>
	
  <nav class="site-state motion-element">
  <!--
      <div class="site-state-item site-state-posts">
        
          <a href="/archives">
        
          <span class="site-state-item-count">105</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
	-->
  </nav>



        </div>
      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Victor Wu</span>
</div>

        












        
      </div>
    </footer>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  </div>

  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.3.0"></script><script src="/js/motion.js?v=7.3.0"></script>
<script src="/js/schemes/pisces.js?v=7.3.0"></script>

<script src="/js/next-boot.js?v=7.3.0"></script>



  





















  

  

  

  


  
  <script src="/js/post-details.js?v=7.3.0"></script>


</body>
</html>
